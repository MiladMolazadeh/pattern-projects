{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PR-imdb-sentiment-analysis-final.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "pnV32AGi3zir",
        "O2Rq9AgFBTRG",
        "QgnrvEh8TjHh",
        "PEGbdtchGW1c"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm2GXYnJAhOY",
        "colab_type": "text"
      },
      "source": [
        "# **Prepare Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niNgIG4RMhe9",
        "colab_type": "code",
        "outputId": "f8e37ea1-8aa3-474e-ea36-1c4e4486c3d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "!pip install bert-embedding\n",
        "!pip install -U sentence-transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-embedding in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: numpy==1.14.6 in /usr/local/lib/python3.6/dist-packages (from bert-embedding) (1.14.6)\n",
            "Requirement already satisfied: mxnet==1.4.0 in /usr/local/lib/python3.6/dist-packages (from bert-embedding) (1.4.0)\n",
            "Requirement already satisfied: gluonnlp==0.6.0 in /usr/local/lib/python3.6/dist-packages (from bert-embedding) (0.6.0)\n",
            "Requirement already satisfied: typing==3.6.6 in /usr/local/lib/python3.6/dist-packages (from bert-embedding) (3.6.6)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet==1.4.0->bert-embedding) (2.21.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet==1.4.0->bert-embedding) (0.8.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (2.8)\n",
            "Requirement already up-to-date: sentence-transformers in /usr/local/lib/python3.6/dist-packages (0.2.3)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.21.3)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.3.1+cu100)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: pytorch-transformers==1.1.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.14.0)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->sentence-transformers) (1.10.14)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->sentence-transformers) (2019.11.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->sentence-transformers) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->sentence-transformers) (0.1.83)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers==1.1.0->sentence-transformers) (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.14.0,>=1.13.14 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers==1.1.0->sentence-transformers) (1.13.14)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers==1.1.0->sentence-transformers) (0.2.1)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.1.0->sentence-transformers) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.1.0->sentence-transformers) (2019.9.11)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.14->boto3->pytorch-transformers==1.1.0->sentence-transformers) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.14->boto3->pytorch-transformers==1.1.0->sentence-transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kdf6vMf1OmhI",
        "colab_type": "code",
        "outputId": "546909ec-2fca-4fd8-b82e-3ddd92ed387c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW7iombu2jzR",
        "colab_type": "code",
        "outputId": "4590dd67-4acc-41a0-ff73-a62820f3c5d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA9ct2I5PCLw",
        "colab_type": "code",
        "outputId": "807f8823-272d-4601-9df0-cde713f23524",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "cd content/drive/My\\ Drive/Colab\\ Notebooks"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6sFBJNx210w",
        "colab_type": "code",
        "outputId": "d7b79051-d6a9-492d-f6da-daff0d7b26f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd .."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdjveJzZbt0Q",
        "colab_type": "code",
        "outputId": "38a9fc75-ede0-4c37-c83e-10c5d8727bcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "# first copy and rename original file\n",
        "# then do\n",
        "import shutil\n",
        "shutil.move(\"aclImdb.tar.gz\", \"/content/\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-5ef645f47de4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"aclImdb.tar.gz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mreal_dst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Destination path '%s' already exists\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: Destination path '/content/aclImdb.tar.gz' already exists"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVxlGzsX58ww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd /"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NocLXHOVe1xh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd /content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvlbOEKq51MY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xf aclImdb.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsNz49qJNQCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from bert_embedding import BertEmbedding\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import html\n",
        "import re\n",
        "import progressbar\n",
        "import itertools \n",
        "import os\n",
        "import random\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzIygMARWj7X",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://i.imgur.com/Mqf7upH.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIQG3b4xfmMc",
        "colab_type": "text"
      },
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFxeeBKMjgXc",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://i.imgur.com/SrKn7zq.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6yEdfXDQBQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# stoplist = set(stopwords.words('english'))\n",
        "stoplist = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    temp = \"\"\n",
        "    words = text.split()\n",
        "    for r in words:\n",
        "        if not r in stoplist:\n",
        "            temp = temp + \" \" + r\n",
        "    return temp.strip()\n",
        "\n",
        "# Stemming\n",
        "def doStem(text):\n",
        "    porter = PorterStemmer()\n",
        "    result = \"\"\n",
        "    for word in text.split():\n",
        "        result = result + \" \" + porter.stem(word)\n",
        "    return result.strip()\n",
        "\n",
        "# Lemmatization\n",
        "def doLemm(text):\n",
        "    wordNet = WordNetLemmatizer()\n",
        "    result = \"\"\n",
        "    for word in text.split():\n",
        "        result = result + \" \" + wordNet.lemmatize(word)\n",
        "    return result.strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_YL5gXJNmrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def camel_case_split(identifier):\n",
        "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
        "    return ' '.join([m.group(0) for m in matches])\n",
        "\n",
        "def preprocess(text,rmvsw,p):\n",
        "\n",
        "    contractions = {\n",
        "        \"ain't\": \"are not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\",\n",
        "        \"could've\": \"could have\", \"couldn't\": \"could not\",\"couldn't've\": \"could not have\", \"didn't\": \"did not\",\n",
        "        \"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\", \"hasn't\": \"has not\",\n",
        "        \"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\n",
        "        \"he'll've\": \"he will have\",\"he's\": \"he is\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\n",
        "        \"how'll\": \"how will\",\"how's\": \"how is\",\"i'd\": \"I would\", \"i'd've\": \"I would have\",\"i'll\": \"I will\",\"i'll've\": \"I will have\",\n",
        "        \"i'm\": \"I am\",\"i've\": \"I have\",\"isn't\": \"is not\",\"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
        "        \"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\n",
        "        \"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n",
        "        \"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\n",
        "        \"shan't\": \"shall not\",\"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\"she'd\": \"she would\", \"she'd've\": \"she would have\",\n",
        "        \"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\"should've\": \"should have\",\n",
        "        \"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so is\",\"that'd\": \"that had\",\"that'd've\": \"that would have\",\n",
        "        \"that's\": \"that is\",\"there'd\": \"there had\", \"there'd've\": \"there would have\", \"there's\": \"there is\",\"they'd\": \"they would\",\n",
        "        \"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\", \"they're\": \"they are\",\n",
        "        \"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\"we'd've\": \"we would have\",\n",
        "        \"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\n",
        "        \"what're\": \"what are\",\"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\"when've\": \"when have\",\"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\"who'll\": \"who will\",\n",
        "        \"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\"will've\": \"will have\",\n",
        "        \"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\n",
        "        \"y'all\": \"you all\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n",
        "        \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\",\"you'd've\": \"you would have\",\"you'll\": \"you will\",\"you'll've\": \"you will have\",\"you're\": \"you are\",\"you've\": \"you have\"\n",
        "    }\n",
        "    if rmvsw == True:\n",
        "      text = remove_stopwords(text)\n",
        "\n",
        "    # CamleCase Split\n",
        "    text = camel_case_split(text).lower() # MiladMolazadeh -> milad molazadeh\n",
        "\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Removing html tags\n",
        "    sentence = remove_tags(text)\n",
        "        \n",
        "    #Apostrophe Lookup\n",
        "    words = sentence.split()\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in contractions:\n",
        "            sentence = sentence.replace(word, contractions[word.lower()])\n",
        "            \n",
        "    sentence = html.unescape(sentence) # remove sth like &lt;3 (that defines heart)\n",
        "    \n",
        "    # happpppppppy -> happy\n",
        "    sentence = ''.join(''.join(s)[:2] for _, s in itertools.groupby(sentence))\n",
        "\n",
        "    # # remove urls\n",
        "    sentence = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', sentence)\n",
        "\n",
        "    \n",
        "    # Single character removal\n",
        "    # sentence = re.sub(r\"\\s+[a-zA-Z]+\\s+\", ' ', sentence)\n",
        "\n",
        "    if p == False:\n",
        "      # Remove punctuations and numbers\n",
        "      sentence = re.sub('[^a-zA-Z\\.\\?\\;\\.\\.+\\!\\!+]', ' ', sentence)\n",
        "      sentence = re.sub('\\.\\.+', '.', sentence)\n",
        "      sentence = re.sub('\\. ', '. ', sentence)\n",
        "      sentence = re.sub('\\?', '. ', sentence)\n",
        "      sentence = re.sub('\\!', '. ', sentence)\n",
        "      sentence = re.sub('\\!\\!+', '. ', sentence)\n",
        "      sentence = re.sub('\\?\\?+', '. ', sentence)\n",
        "      sentence = re.sub('\\;\\;+', '. ', sentence)\n",
        "      sentence = re.sub('\\:\\:+', ' ', sentence)\n",
        "      sentence = re.sub('[\\']', ' ', sentence)\n",
        "      sentence = doStem(sentence)\n",
        "      sentence = doLemm(sentence)\n",
        "      # Removing multiple spaces\n",
        "      sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "      return \" \".join(sentence.split()).lower()\n",
        "    \n",
        "    else:\n",
        "      sentence = re.sub('[^a-zA-Z\\']', ' ', sentence)\n",
        "      sentence = re.sub('\\.\\.+', ' ', sentence)\n",
        "      sentence = re.sub('\\. ', ' ', sentence)\n",
        "      sentence = re.sub('\\!\\!+', ' ', sentence)\n",
        "      sentence = re.sub('\\?\\?+', ' ', sentence)\n",
        "      sentence = re.sub('\\;\\;+', ' ', sentence)\n",
        "      sentence = re.sub('\\:\\:+', ' ', sentence)\n",
        "      sentence = re.sub('[\\']', ' ', sentence)\n",
        "      # Single character removal\n",
        "      text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "      sentence = doStem(sentence)\n",
        "      # sentence = doLemm(sentence)\n",
        "      \n",
        "      # Removing multiple spaces\n",
        "      sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "      return \" \".join(sentence.split()).lower()\n",
        "\n",
        "\n",
        "\n",
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "\n",
        "def remove_tags(text):\n",
        "    return TAG_RE.sub('', text)\n",
        "\n",
        "sample=\"mr.ali how not are you? hi AwsomeBookFind\"\n",
        "\n",
        "preprocess(sample,True,True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9WIbBbOSJBh",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://i.imgur.com/CVC0WeL.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-k5P5VM9PkU",
        "colab_type": "text"
      },
      "source": [
        "داکبومنت ها را شافل میکنیم تا مشکل پیش برازش پیش نیاید\n",
        "\n",
        "و نتیجه ما تحت تاثیر مرتب بودن دیتا نباشد"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOEN_sX9Nvvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "def load_dataset_to_python(datapath, seed,remove_stopwords, sentence,p):\n",
        "    imdb_data_path = os.path.join(datapath,\"aclImdb\")\n",
        "    train_texts = []\n",
        "    train_labels = []\n",
        "    \n",
        "    for category in ['pos', 'neg']:\n",
        "        train_path = os.path.join(imdb_data_path, 'train', category)\n",
        "        for fname in sorted(os.listdir(train_path)):\n",
        "            if fname.endswith('.txt'):\n",
        "                with open(os.path.join(train_path, fname)) as f:\n",
        "                        train_texts.append(preprocess(f.read(),True,p))\n",
        "                        train_labels.append(0 if category == 'neg' else 1)\n",
        "\n",
        "    test_texts = []\n",
        "    test_labels = []\n",
        "    \n",
        "    for category in ['pos', 'neg']:\n",
        "        test_path = os.path.join(imdb_data_path, 'test', category)\n",
        "        for fname in sorted(os.listdir(test_path)):\n",
        "            if fname.endswith('.txt'):\n",
        "                with open(os.path.join(test_path, fname)) as f:\n",
        "                    test_texts.append(preprocess(f.read(),True,p))\n",
        "                    test_labels.append(0 if category == 'neg' else 1)\n",
        "                \n",
        "    random.seed(seed)\n",
        "    random.shuffle(train_texts)\n",
        "    random.seed(seed)\n",
        "    random.shuffle(train_labels)\n",
        "\n",
        "    \n",
        "    return ((np.array(train_texts), np.array(train_labels)), (np.array(test_texts), np.array(test_labels)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUPH1Ld2CNoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "train , test = load_dataset_to_python(\"\", seed=200,remove_stopwords=False,sentence=False,p=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lnl6pnH8df_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "train_dot , test_dot = load_dataset_to_python(\"\", seed=200,remove_stopwords=False,sentence=False,p=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk5TkId4Qvo_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "train_sw , test_sw = load_dataset_to_python(\n",
        "    \"\", seed=200,remove_stopwords=True,sentence=False,p=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcZsAc2wdkJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "train_sw_dot , test_sw_dot = load_dataset_to_python(\n",
        "    \"\", seed=200,remove_stopwords=True,sentence=False,p=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DebJqDjz6aqZ",
        "colab_type": "text"
      },
      "source": [
        "# **Vectorize**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Phvy7SzDUzN_",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://i.imgur.com/MvcU0pZ.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dCUfRN1NZCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bag_of_words_vectorize(lstTrain,lstTest):\n",
        "    cv = CountVectorizer()\n",
        "    cv.fit(lstTrain)\n",
        "    X = cv.transform(lstTrain)\n",
        "    X_test = cv.transform(lstTest)\n",
        "    \n",
        "    print(r\"Done! number of features are: \",len(cv.get_feature_names()))\n",
        "    print(r\"type of outputs are: \",type(X))\n",
        "\n",
        "    return X,X_test\n",
        "\n",
        "\n",
        "def tfidf_vectorize(lstTrain,lstTest,ngram=(1,3)):\n",
        "    cv = TfidfVectorizer(binary=False, ngram_range=ngram)\n",
        "    cv.fit(lstTrain)\n",
        "    X = cv.transform(lstTrain)\n",
        "    X_test = cv.transform(lstTest)\n",
        "    \n",
        "    print(r\"Done! number of features are: \",len(cv.get_feature_names()))\n",
        "    print(r\"type of outputs are: \",type(X))\n",
        "    \n",
        "    return X,X_test\n",
        "\n",
        "def word2vec(lstSentences):\n",
        "    vectors = []\n",
        "    for sentence in lstSentences:\n",
        "      tokens = [nltk.word_tokenize(sentence)]\n",
        "      word_vec = Word2Vec(tokens, min_count=1)\n",
        "      vocabulary = word_vec.wv.vocab\n",
        "      i=0\n",
        "      for word in vocabulary:\n",
        "        if i == 0:\n",
        "          i=1\n",
        "          v1 = word_vec.wv[word]\n",
        "        else:\n",
        "          v1 = sum_two_matrix(v1,word_vec.wv[word])\n",
        "      vectors.append([x / len(vocabulary) for x in v1])\n",
        "    return vectors\n",
        "\n",
        "def bert_word_embedding(lstParagraphs):\n",
        "    vectors = []\n",
        "    v1 = []\n",
        "\n",
        "    bert_embedding = BertEmbedding()\n",
        "    return bert_embedding(lstParagraphs)\n",
        "    for par in lstParagraphs:\n",
        "      lstSent = par.split('. ')\n",
        "      i=0\n",
        "      for sent in lstSent:\n",
        "        if i == 0:\n",
        "          j = 0\n",
        "          for j in len(v0):\n",
        "            if j == 0:\n",
        "              v0 = bert_embedding(sent)[i][1][j]\n",
        "              j=1\n",
        "            else:\n",
        "              v0 = sum_two_matrix(v0,bert_embedding(sent)[i][1][j])\n",
        "              j = j+1\n",
        "          v1.append([x / len(v0) for x in v0])\n",
        "          i = i + 1\n",
        "        else:\n",
        "          j = 0\n",
        "          for j in len(v0):\n",
        "            if j == 0:\n",
        "              v0 = bert_embedding(sent)[i][1][j]\n",
        "              j=1\n",
        "            else:\n",
        "              v0 = sum_two_matrix(v0,bert_embedding(sent)[i][1][j])\n",
        "              j = j+1\n",
        "          v1.append([x / len(v0) for x in v0])\n",
        "          i = i + 1\n",
        "      vectors.append([x / len(lstSent) for x in v1])\n",
        "    return vectors\n",
        "\n",
        "def sentence_word_embedding(lstTrain, lstTest):\n",
        "    \n",
        "    model = SentenceTransformer('bert-base-nli-mean-tokens')    \n",
        "    return model.encode(lstTrain),model.encode(lstTest)\n",
        "\n",
        "def sum_two_matrix(X,Y):\n",
        "  return [X[i] + Y[i]  for i in range(len(X))]\n",
        "\n",
        "# result = bert_word_embedding(['my name is milad', 'i live in iran'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvmGUc906ShG",
        "colab_type": "text"
      },
      "source": [
        "# **Classifiers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBsLrNBuYp3N",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://i.imgur.com/2YxiRM1.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v_v9Bo-DBGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnAjs1zy-hVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_curve\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_roc_curve(fpr, tpr, label=None):\n",
        "  plt.plot(fpr,tpr, linewidth=3, label=label)\n",
        "  plt.plot([0, 1], [0, 1], 'k--')\n",
        "  plt.axis([0, 1, 0, 1])\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "\n",
        "def modelEvaluation(predictions, y_test_set):\n",
        "    #Print model evaluation to predicted result \n",
        "    \n",
        "    print (\"\\nAccuracy on test set: {:.4f}\".format(accuracy_score(y_test_set, predictions)))\n",
        "    print(\"---------------------------\")\n",
        "    print (\"\\nAUC score : {:.4f}\".format(roc_auc_score(y_test_set, predictions)))\n",
        "    print(\"---------------------------\")\n",
        "    print (\"\\nClassification report : \\n\", metrics.classification_report(y_test_set, predictions))\n",
        "    print(\"---------------------------\")\n",
        "    print (\"\\nConfusion Matrix : \\n\", metrics.confusion_matrix(y_test_set, predictions))\n",
        "    print(\"\\n---------------------------\\n\")\n",
        "    fpr, tpr, tresholds = roc_curve(predictions, y_test_set)\n",
        "    plot_roc_curve(fpr,tpr)\n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "def naive_bayes_classifier(train, test, train_target, test_target):\n",
        "    mnb = MultinomialNB()\n",
        "    mnb.fit(train, train_target)\n",
        "    predictions = mnb.predict(test)\n",
        "    modelEvaluation(predictions, test_target)\n",
        "    \n",
        "    return None\n",
        "\n",
        "def naive_bayes_classifier_gausianNB(train, test, train_target, test_target):\n",
        "    gnb = GaussianNB()\n",
        "    gnb.fit(train, train_target)\n",
        "    predictions = gnb.predict(test)\n",
        "    modelEvaluation(predictions, test_target)\n",
        "    \n",
        "    return None\n",
        "\n",
        "def svm_classifier(train, test, train_target, test_target):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(train, train_target, train_size=0.75) \n",
        "    final = LinearSVC(C=0.5)\n",
        "    final.fit(train, train_target)\n",
        "    modelEvaluation(final.predict(test), test_target)\n",
        "                                    \n",
        "    return None\n",
        "\n",
        "\n",
        "def random_forest_classifier(train, test, train_target, test_target):\n",
        "    rand = RandomForestClassifier() # n_estimators is 10 \n",
        "    rand.fit(train, train_target)\n",
        "    pred = rand.predict(test)\n",
        "    modelEvaluation(pred, test_target)\n",
        "    return None\n",
        "\n",
        "\n",
        "def dt_classifier(train,test, train_target,test_target):\n",
        "    dt = DecisionTreeClassifier()\n",
        "    dt.fit(train,train_target)\n",
        "    pred = dt.predict(test)\n",
        "    modelEvaluation(pred, test_target)\n",
        "    return None\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcmlXOPgZbFp",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o58BXRIzZTAY",
        "colab_type": "text"
      },
      "source": [
        "# **Bag of Words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4NMD4nzNlT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "X, X_test = bag_of_words_vectorize(train[0],test[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NCDY-X8-Tgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "svm_classifier(X, X_test, train[1], test[1])\n",
        "# 0.8472 => entire document\n",
        "# 0.8486 => sentence by sentence\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvDTaBrrVge7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "naive_bayes_classifier(X, X_test, train[1], test[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fq0_25qCALpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "random_forest_classifier(X,X_test,train[1],test[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-TGpeA1ARhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "dt_classifier(X,X_test,train[1],test[1])\n",
        "# 0.7118 ver2 entire\n",
        "# 0.8152 => sentence by sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnV32AGi3zir",
        "colab_type": "text"
      },
      "source": [
        "# **Word2Vec**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTvLDQIy0q0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "par = \"hi may name is milad. i live iniran.\"\n",
        "sentences = par.split('. ')\n",
        "print(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kPBTiU52sHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word2vec sentence by sentence\n",
        "%%time\n",
        "train_vectors_snt = []\n",
        "\n",
        "for par in train[0]:\n",
        "  i=0\n",
        "  sentences = par.split('. ')\n",
        "  if i == 0:\n",
        "    i=1\n",
        "    sent_vector = word2vec(filter(None, sentences))\n",
        "  else:\n",
        "    sent_vector = sum_two_matrix(sent_vector,word2vec(sentences))\n",
        "  train_vectors_snt.append(sent_vector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkXZi_TxUskO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(train_vectors_snt))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umOwKrskHSzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word2vec sentence by sentence\n",
        "%%time\n",
        "test_vectors_snt = []\n",
        "\n",
        "for par in test[0]:\n",
        "  i=0\n",
        "  sentences = par.split('. ')\n",
        "  if i == 0:\n",
        "    i=1\n",
        "    sent_vector = word2vec(filter(None, sentences))\n",
        "  else:\n",
        "    np.sum(sent_vector,word2vec(sentences))\n",
        "  test_vectors_snt.append(sent_vector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ViZV9PfeWfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "train_sw_dot_vectors = word2vec(train_sw_dot[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv7bhAh4efJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "test_sw_dot_vectors = word2vec(test_sw_dot[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFWAJd2Hkica",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "train_vectors = word2vec(train_dot[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce8hVWolkiMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "test_vectors = word2vec(test_dot[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPFSxbFwkxYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "svm_classifier(train_vectors, test_vectors,train[1], test[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu6M8b-g39dg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "svm_classifier(train_sw_dot_vectors, test_sw_dot_vectors,train_sw_dot[1], test_sw_dot[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o381gbVbtNDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# naive bayes with word2vec paragraph by paragraph\n",
        "\n",
        "%%time\n",
        "naive_bayes_classifier_gausianNB(np.asarray(train_vectors), np.asarray(test_vectors),train[1], test[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az9sx59PtSnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "naive_bayes_classifier_gausianNB(train_sw_dot_vectors, test_sw_dot_vectors,train_sw_dot[1], test_sw_dot[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qk-BlM7p4fKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# random forest with word2vec paragraph by paragraph\n",
        "\n",
        "%%time\n",
        "random_forest_classifier(train_vectors, test_vectors,train[1], test[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9wOUz4qtfH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "random_forest_classifier(train_sw_dot_vectors, test_sw_dot_vectors,train_sw_dot[1], test_sw_dot[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8znbKpP9uFxo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# decision tree with  word2vec paragraph by paragraph\n",
        "\n",
        "%%time\n",
        "dt_classifier(train_vectors, test_vectors,train[1], test[1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDtf2jgvt0cf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "dt_classifier(train_sw_dot_vectors, test_sw_dot_vectors,train_sw_dot[1], test_sw_dot[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2Rq9AgFBTRG",
        "colab_type": "text"
      },
      "source": [
        "# **BERT Sentence embedding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T1F-HNBBXvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "X_wbert, X_wbert_test = sentence_word_embedding(train[0],test[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qflMXieFMLqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "svm_classifier(X_wbert, X_wbert_test, train[1], test[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaD6tl9wTe4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "naive_bayes_classifier_gausianNB(X_wbert, X_wbert_test, train[1], test[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tCp7cQggX4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "random_forest_classifier(X_wbert, X_wbert_test, train[1], test[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okuj1Vm6ggC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "dt_classifier(X_wbert, X_wbert_test, train[1], test[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgnrvEh8TjHh",
        "colab_type": "text"
      },
      "source": [
        "# **TF-IDF**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB92SQox6IJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "train_tfidf,test_tfidf = tfidf_vectorize(train[0],test[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXgZjA56SxKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "train_tfidf_sw,test_tfidf_sw = tfidf_vectorize(\n",
        "    train_sw[0],test_sw[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN91ZeO07gsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svm_classifier(train_tfidf, test_tfidf,train[1], test[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKPvLBAqF27S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svm_classifier(train_tfidf_sw, \n",
        "               test_tfidf_sw,\n",
        "               train_sw[1], \n",
        "               test_sw[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP9tuSky8Bsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "naive_bayes_classifier(train_tfidf, test_tfidf,train[1], test[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG7qNPN7GM8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "naive_bayes_classifier(train_tfidf_sw, \n",
        "               test_tfidf_sw,\n",
        "               train_sw[1], \n",
        "               test_sw[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6Kwqa-d8C9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_forest_classifier(train_tfidf, test_tfidf,train[1], test[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jkk1Ozx_Gsud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_forest_classifier(train_tfidf_sw, \n",
        "               test_tfidf_sw,\n",
        "               train_sw[1], \n",
        "               test_sw[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2icgDiB-0BY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dt_classifier(train_tfidf, test_tfidf,train[1], test[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYXA-klQVLCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "dt_classifier(train_tfidf_sw, \n",
        "               test_tfidf_sw,\n",
        "               train_sw[1], \n",
        "               test_sw[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGnYfOs3VK4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_nyoLR399OY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEGbdtchGW1c",
        "colab_type": "text"
      },
      "source": [
        "# Bert Word Embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7hhL6GKGd5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_wordBert = bert_word_embedding(train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUgUI8Ne1VdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_wordBert_test = bert_word_embedding(test[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2R7z_Nfct3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "naive_bayes_classifier(X_wbert, X_wbert_test,train[1], test[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT5Gi5APc6__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svm_classifier(X_wbert, X_wbert_test,train[1], test[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66eZNj4Vc6zl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_forest_classifier(X_wbert, X_wbert_test,train[1], test[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8OkRwx6c6ku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dt_classifier(X_wbert, X_wbert_test,train[1], test[1])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}